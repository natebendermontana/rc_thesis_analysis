---
title: "Thesis Analysis"
author: "Nate Bender"
date: "2/16/2022"
note: "Nationally-representative survey; conducted Aug 3-9, 2020"
output: html_document
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE)
```

```{r load packages and data, eval=T, echo=F, include=F}
### Packages / libraries / load data & filter rejected out 
library(DescTools)
library(broom)
library(brant)
library(generalhoslem)
library(tidyr)
library(lmtest)  #lrtest
library(nortest)  #sf.test
library(ResourceSelection)  #hoslem.test
library(nnet)
library(oglmx)

library(psych)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(GGally)
library(ggfortify)
library(ggcorrplot)
library(ggpubr)

library(GPArotation)
library(ltm)
library(fBasics)
library(car)
library(fastDummies)
library(MASS)
library(Hmisc)
library(ROCR)
library(plotROC)
library(pROC)
library(pscl)
library(blorr)

```

## Outline
This document serves two functions: as Nate's detailed reference for steps and assumptions taken during analysis, and more broadly to communicate the analysis findings to advisors. Feel free to skip granular sections as needed.   

The analysis seeks to answer my thesis research questions:  
  
RQ1: Which explanatory variables from a variety of studies and theories are most strongly associated with climate activism, specifically the action of contacting elected officials?   
  
RQ2: How might we define behaviorally oriented segments and which segments are specifically related to the climate change activism behavior “contacting elected officials”?

At this point, the code only extends to RQ1. 

Table of contents:  
5) Regression modelling approaches

```{r, eval=T, echo=F, include=T}
regression_clean <- read.csv("/Users/natebender/Desktop/Repo/RCthesisanalysis/cleandata/perenial_complete_for_analysis.csv", header=TRUE, stringsAsFactors = TRUE)

```

```{r, eval=T, echo=F, include=T}
# Build initial ordinal logistic regression model with all potential variables included
levels(regression_clean$sr_12a_actions_contacted_officials)
regression_clean$sr_12a_actions_contacted_officials_binary <- recode_factor(regression_clean$sr_12a_actions_contacted_officials, "nocontact"="0", "once"="1", "morethanonce"="1")

regression_clean$sr_12a_actions_contacted_officials_binary <- as.character(regression_clean$sr_12a_actions_contacted_officials_binary)
str(regression_clean$sr_12a_actions_contacted_officials_binary)

regression_clean$sr_12a_actions_contacted_officials_binary <- as.numeric(regression_clean$sr_12a_actions_contacted_officials_binary)
str(regression_clean$sr_12a_actions_contacted_officials_binary)

regression_clean <- regression_clean %>% 
  mutate(sr_12a_actions_contacted_officials_binary = factor(sr_12a_actions_contacted_officials_binary, ordered=F))

describe(regression_clean$sr_12a_actions_contacted_officials_binary)

```
### Partitioning into training/test data
```{r, eval=T, echo=F, include=F}
# # Training set: 75% of the sample size
# smp_size <- floor(0.75 * nrow(regression_clean))
# 
# ## set the seed to make your partition reproducible
# set.seed(123456)
# train_ind <- sample(seq_len(nrow(regression_clean)), size = smp_size)
# 
# train <- regression_clean[train_ind, ]
# test <- regression_clean[-train_ind, ]

```

```{r, eval=T, echo=F, include=F}
# set reference groups for regression
# train$race_white_dumvar <- relevel(train$race_white_dumvar, ref = "race_white")
# train$children_dumvar <- relevel(train$children_dumvar, ref = "children")
# train$sr_75_religion_dumvar <- relevel(train$sr_75_religion_dumvar, ref = "religious")
# train$gender_dumvar <- relevel(train$gender_dumvar, ref = "male")
# train$sr_56_marital_status <- relevel(train$sr_56_marital_status, ref = "married_partner")
# train$sr_61_education <- relevel(train$sr_61_education, ref = "assocdeg_orlower")
# train$sr_71_employment_status <- relevel(train$sr_71_employment_status, ref = "workfull")
# train$sr_72_income <- relevel(train$sr_72_income, ref = "under100k")
# train$sr_79_political_leaning <- relevel(train$sr_79_political_leaning, ref = "liberal")
# train$sr_7_believe_about_climate_change <- relevel(train$sr_7_believe_about_climate_change, ref = "excl_human")

```



```{r, eval=T, echo=F, include=T}
# TEST - USING THE CARET PACKAGE TO MODEL SO THAT I CAN MORE SEAMLESSLY TRANSFER MODEL PREDICTIONS INTO 'MODELPLOTR' FUNCTION
library(devtools)
library(modelplotr)
library(caret)
library(glmnet)
library(randomForest)


# reduce dataset down to just variables needed for saturated model, in order to make code later on much cleaner. 
data_for_regression <- regression_clean %>% 
  select(
    matches("sr_12a_actions_contacted_officials_binary"),
    matches("age_true"),
    matches("race_white_dumvar"),
    matches("gender_dumvar"),
    matches("children_dumvar"),
    matches("sr_75_religion_dumvar"),
    matches("sr_56_marital_status"),
    matches("sr_61_education"),
    matches("sr_71_employment_status"),
    matches("sr_72_income"),
    matches("sr_79_political_leaning"),
    matches("sr_7_believe_about_climate_change"),
    matches("descdynamicnorms_comp"),
    matches("desccontactnorms_all_comp"),
    matches("descrolemodelnorms_all_comp"),
    matches("injunctcontactnorms_all_comp"),
    matches("injunctmotivation_all_comp"),
    matches("cimbenefits_comp"),
    matches("cimperceivedrisk_comp"),
    matches("sr_10_harm_you_personally_reversed"),
    matches("sr_11_harm_future_generations_reversed"),
    matches("sr_21a_effective_actions_contacting_officials"),
    matches("efficacy_effectiveness_all_comp"),
    matches("efficacy_competresp_all_comp"),
    matches("behatt_admirablegood_comp"),
    matches("behatt_usefulpleasantsensible_comp"),
    matches("behatt_coolexcitingeasy_comp"),
    matches("sr_30_easy_to_call"),
    matches("sr_31_able_to_call"),
    matches("sr_41a_right_to_modify"),
    matches("sr_41b_laws_of_nature"),
    matches("sr_41c_ingenuity"),
    matches("sr_41d_impotent"),
    matches("sr_41e_govt_do_more"),
    matches("sr_41f_equity")
    )

set.seed(19881)
default_idx = createDataPartition(data_for_regression$sr_12a_actions_contacted_officials_binary, p = 0.75, list = FALSE)
default_train = data_for_regression[default_idx, ]
default_test = data_for_regression[-default_idx, ]

# set reference groups for regression
default_train$race_white_dumvar <- relevel(default_train$race_white_dumvar, ref = "race_white")
default_train$children_dumvar <- relevel(default_train$children_dumvar, ref = "children")
default_train$sr_75_religion_dumvar <- relevel(default_train$sr_75_religion_dumvar, ref = "religious")
default_train$gender_dumvar <- relevel(default_train$gender_dumvar, ref = "male")
default_train$sr_56_marital_status <- relevel(default_train$sr_56_marital_status, ref = "married_partner")
default_train$sr_61_education <- relevel(default_train$sr_61_education, ref = "assocdeg_orlower")
default_train$sr_71_employment_status <- relevel(default_train$sr_71_employment_status, ref = "workfull")
default_train$sr_72_income <- relevel(default_train$sr_72_income, ref = "under100k")
default_train$sr_79_political_leaning <- relevel(default_train$sr_79_political_leaning, ref = "liberal")
default_train$sr_7_believe_about_climate_change <- relevel(default_train$sr_7_believe_about_climate_change, ref = "excl_human")

```

### logistic regression. model selection using AIC w/ backwards selection
```{r, eval=T, echo=F, include=F}
# 35 variables total
# removed sr_78 political affiliation b/c highly correlated with sr_79 ideology

logit_contacted <- glm(sr_12a_actions_contacted_officials_binary ~
                        age_true + race_white_dumvar + gender_dumvar + children_dumvar +
                        sr_75_religion_dumvar + sr_56_marital_status + sr_61_education +
                        sr_71_employment_status + sr_72_income +
                        sr_79_political_leaning + sr_7_believe_about_climate_change +
                        descdynamicnorms_comp + desccontactnorms_all_comp +
                        descrolemodelnorms_all_comp + injunctcontactnorms_all_comp +
                        injunctmotivation_all_comp + cimbenefits_comp +
                        cimperceivedrisk_comp + sr_10_harm_you_personally_reversed +
                        sr_11_harm_future_generations_reversed +
                        sr_21a_effective_actions_contacting_officials +
                        efficacy_effectiveness_all_comp + efficacy_competresp_all_comp +
                        behatt_admirablegood_comp + behatt_usefulpleasantsensible_comp +
                        behatt_coolexcitingeasy_comp +
                        sr_30_easy_to_call + sr_31_able_to_call + sr_41a_right_to_modify +
                        sr_41b_laws_of_nature + sr_41c_ingenuity + sr_41d_impotent +
                        sr_41e_govt_do_more + sr_41f_equity, data = default_train, family=binomial)


logit_contacted_aic <- stepAIC(logit_contacted, direction="backward")

# pred_test <- predict(logit_contacted_aic,test,type="response")
# ROCR_pred_test <- prediction(pred_test,test$sr_12a_actions_contacted_officials_binary)
# perf <- performance(ROCR_pred_test,"lift","rpp")
# plot(perf, main="Lift curve", colorize=F)           # our baseline model is 0.75 and the actual lift is plotted as below
```

```{r}
summary(logit_contacted_aic)
```


```{r}
# Create decile-wise lift chart
gt <- blr_gains_table(logit_contacted_aic)
blr_decile_lift_chart(
  gt,
  xaxis_title = "Decile",
  yaxis_title = "Decile Mean / Global Mean",
  title = "Decile Lift Chart",
  bar_color = "blue",
  text_size = 3.5,
  text_vjust = -0.3,
  print_plot = TRUE
)

# the kappa statistic is a measure of how closely the instances classified by the machine learning classifier matched the data labeled as ground truth, controlling for the accuracy of a random classifier as measured by the expected accuracy
blr_confusion_matrix(logit_contacted_aic)

```


### Graphing logistic regression
```{r}

m1_log_preds = tidy(logit_contacted_aic, conf.int = T) %>%
    mutate(Model = "Past contact")
m1_log_predstest <- subset(m1_log_preds, term !="(Intercept)" & p.value < 0.05)  # remove intercept
m1_log_predstest <- m1_log_predstest %>%
 mutate(term = c("Descriptive contact norms", "Injunctive contact norms", "CIM benefits", "CC personal harm", "CC harm future generations","Efficacy: able to call"))

# Specify the width of your confidence intervals
interval1 <- -qnorm((1-0.9)/2)  # 90% multiplier
interval2 <- -qnorm((1-0.95)/2)  # 95% multiplier

# Plot
#pdf("ThesisPastaction_plot_p_05.pdf") # starts writing a PDF to file
#png("ThesisPastaction_plot_p_05.png") # starts writing a PDF to file
zp1 <- ggplot(m1_log_predstest, aes(colour = Model))
zp1 <- zp1 + geom_hline(yintercept = 0, colour = gray(1/2), lty = 2)
zp1 <- zp1 + geom_linerange(aes(x = term, ymin = conf.low,
                                ymax = conf.high),
                            lwd = 1, position = position_dodge(width = 1/2))
zp1 <- zp1 + geom_pointrange(aes(x = term, y = estimate, ymin = conf.low,
                                 ymax = conf.high),
                             lwd = 1/2, position = position_dodge(width = 1/2),
                             shape = 21, fill = "WHITE")
zp1 <- zp1 + coord_flip() + theme_bw()
INTzp1_log <- zp1 + ggtitle("What influences past representative contact?") + ylab("Odds Ratio") + xlab("Variable")
print(INTzp1_log)  # The trick to these is position_dodge().
dev.off()

write.csv(m1_log_predstest,"/Users/natebender/Desktop/ThesisPastactionregmodel.csv", row.names = TRUE)

```

```{r}
print(INTzp1_log) # prints plot to screen after it's been saved to file
```

```{r}
summary(logit_contacted_aic)
```




### Logistic regression - cluster analysis
```{r, eval=T, echo=F, include=T}
evs_forclustering <- regression_clean %>% 
  select(desccontactnorms_all_comp, injunctcontactnorms_all_comp, cimbenefits_comp, sr_10_harm_you_personally_reversed, sr_11_harm_future_generations_reversed, sr_31_able_to_call)

res <- cor(evs_forclustering, use = "complete.obs")
round(res, 2)
# .77 corr b/w injunctive and descriptive norms - too much?
```

```{r}
# 4 clusters
kmean_4 <- kmeans(evs_forclustering, 4)  # test three groups
kmean_4$centers
kmean_4
autoplot(kmean_4, na.omit(evs_forclustering), frame = TRUE)

```

```{r}
kmean_2
```



```{r, eval=T, echo=F, include=T}

kmean_2 <- kmeans(evs_forclustering, 2)  # test two groups
kmean_2$centers

autoplot(kmean_2, na.omit(evs_forclustering), frame = TRUE)

# 3 cluster solution
kmean_3 <- kmeans(evs_forclustering, 3)  # test three groups
kmean_3$centers
kmean_3

autoplot(kmean_3, evs_forclustering, frame = TRUE)




# 5 clusters
kmean_5 <- kmeans(evs_forclustering, 5)  # test three groups
kmean_5$centers
kmean_5

autoplot(kmean_5, evs_forclustering, frame = TRUE)
```
```{r}
library(factoextra)

# elbow method suggests 3 clusters.
# Best balance between minimizing WSS while keeping total clusters to a reasonable number. 
# The location of a bend (knee) in the plot is generally considered as an indicator of the appropriate number of clusters.
fviz_nbclust(evs_forclustering, kmeans, method = "wss") +
geom_vline(xintercept = 3, linetype = 2)
```


```{r}
# Average silhouette method
# the average silhouette approach measures the quality of a clustering. That is, it determines how well each object lies within its cluster. A high average silhouette width indicates a good clustering.

# This method recommends two clusters
fviz_nbclust(evs_forclustering, kmeans, method = "silhouette")
```
```{r}
# Gap statistic
# The gap statistic compares the total intracluster variation for different values of k with their expected values under null reference distribution of the data (i.e. a distribution with no obvious clustering)

# nstart option attempts multiple initial configurations and reports on the best one. For example, adding nstart=25 will generate 25 initial random centroids and choose the best one for the algorithm.
# kmax sets upper limit of clusters to test against
# B is the number of bootstrapped samples the function uses to test against. The reference dataset is generated using Monte Carlo simulations of the sampling process
library(cluster)
gap_stat <- clusGap(evs_forclustering, FUN = kmeans, nstart = 10,
                    K.max = 5, B = 20)
# gap stat suggests three clusters
fviz_gap_stat(gap_stat)

```
```{r}
install.packages("NbClust")
library(NbClust)
nb <- NbClust(evs_forclustering, distance = "euclidean", min.nc = 2,
        max.nc = 10, method = "kmeans")

fviz_nbclust(nb)
```

```{r}
library(GGally)
library(plotly)

regression_clean$cluster <- as.factor(kmean_4$cluster)

df4_clus_avg <- regression_clean %>%
  group_by(cluster) %>%
  summarize_if(is.numeric, mean)

p <- ggparcoord(
  data = df4_clus_avg, 
  columns = c(5,7, 9, 11, 12, 21), 
  groupColumn = "cluster", 
  scale = "globalminmax", # no variable scaling is done; the range of the graphs is defined by the global minimum and the global maximum
  order = "skewness") + 
  labs(x = "Predictive variables", 
       y = "value", 
       title = "Clustering") +
  theme(axis.text.x=element_text(angle=7, hjust=1))

ggplotly(p)
```


```{r}
norms_benes <- c(5,7, 9)
harm_efficacy <- c(11, 12, 21)

regression_clean %>% 
  pivot_longer(cols = norms_benes,
               names_to = "question", 
               values_to = "response") %>% 
  ggplot(aes(x = response, colour = cluster)) +
  facet_wrap(vars(question), ncol = 1) +
  geom_point(stat = "count") +
  geom_line(stat = "count") +
  labs(x = "Response", y = "Number of respondents")

```
```{r}
regression_clean %>% 
  pivot_longer(cols = harm_efficacy,
               names_to = "question", 
               values_to = "response") %>% 
  ggplot(aes(x = response, colour = cluster)) +
  facet_wrap(vars(question), ncol = 1) +
  geom_point(stat = "count") +
  geom_line(stat = "count") +
  labs(x = "Response", y = "Number of respondents")
```



```{r}
regression_clean %>%
  mutate(Cluster = kmean_4$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")
```
#### Merging targeting variables to clusters
```{r}
original_data <- read.csv("/Users/natebender/Desktop/Repo/RCthesisanalysis/dyn_national_20200828 - results-20200930-082116_NEW.csv", header=TRUE, stringsAsFactors = TRUE)

targeting_data <- original_data %>% 
  select(
    matches("")
  )

merged_data <- merge(regression_clean2, original_data, by = 'respondent_id', all.x= F)







```





### IGNORE FOR THESIS - USING CARET PACKAGE FOR MODELING
```{r, eval=T, echo=F, include=T}
# 35 variables including RV
# 
# ctrl <- trainControl(method = "cv", number = 5)  # five cross-validations
# 
# # saturated logistic regression
# logit_contacted_saturated <- caret::train(
#   sr_12a_actions_contacted_officials_binary ~ .,
#   data = default_train,
#   method = "glm",
#   family = "binomial",
#   trControl = ctrl
#   )
# 
# # Logistic regression via backwards stepwise AIC selection
# logit_contacted_test <- caret::train(
#   sr_12a_actions_contacted_officials_binary ~ .,
#   data = default_train,
#   method = "glmStepAIC",
#   direction = "backward",
#   family = "binomial",
#   trControl = ctrl
#   )
# 
# # Multinomial logistic regression via glmnet package
# # Extension of glm models with built-in variable selection. Uses both lasso and ridge penalization.
# mnl <- caret::train(
#   sr_12a_actions_contacted_officials_binary ~ .,
#   data = default_train, 
#   method = "glmnet",
#   trControl = ctrl
#   )
# 
# 
# # Random forest model 
# # random forests are an ensemble learning method for classification and regression that operate by constructing a lot of decision trees at training time and outputting the class that is the mode of the classes output by individual trees.
# rf <- caret::train(
#   sr_12a_actions_contacted_officials_binary ~ .,
#   data = default_train,
#   method = "rf",
#   trControl = ctrl
# )
# 
# 
# # # XGBoost
# # # xgboost only deals with numeric data — how best to manipulate data? Force all factors to numeric?
# # library(mlr)
# # library(xgboost)
# # 
# # traintask <- makeClassifTask(data = default_train, target = "sr_12a_actions_contacted_officials_binary", positive = 1)
# # testtask <- makeClassifTask(data = default_test, target = "sr_12a_actions_contacted_officials_binary")
# # 
# # xgb_learner <- makeLearner(
# #   "classif.xgboost",
# #   predict.type = "response",
# #   par.vals = list(
# #     objective = "binary:logistic",
# #     eval_metric = "error",
# #     nrounds = 200
# #   )
# # )
# # 
# # xgb_model <- train(xgb_learner, task = traintask)
# 


```

```{r}
# summary(logit_contacted_test)
```


```{r}
# prepared_input <- prepare_scores_and_ntiles(datasets=list("default_train","default_test"),
#   dataset_labels = list("train data","test data"),
#   models = list("logit_contacted_test", "mnl", "rf"),
#   model_labels = list("LogitStepAIC", "Glmnet", "Randomforest"),
#   target_column= "sr_12a_actions_contacted_officials_binary")
# 
# forplot <- plotting_scope(
#   prepared_input,
#   scope = "compare_models",
#   select_dataset_label = "test data", 
#   select_targetclass = NA)
# 
# forplotindividual <- plotting_scope(
#   prepared_input,
#   select_model_label = "LogitStepAIC", 
#   select_dataset_label = "test data", 
#   select_targetclass = NA)
# 
# # all four plots together
# plot_multiplot(
#   data = forplotindividual
# )
# 
# # just cumulative lift 
# plot_cumlift(
#   data = forplotindividual,
#   highlight_ntile = 2,
# )
# 
# # just cumulative response
# plot_cumresponse(
#   data = forplot,
#   highlight_ntile = 2
# )

```


```{r}
# # random forest interpretation
# plot(rf)
# 
# rf_predictions <- predict(rf, default_test)
# 
# # "accuracy" is accuracy on training data. 
# confusionMatrix(table(default_test[,"sr_12a_actions_contacted_officials_binary"],rf_predictions))
# 
# varImp(rf)
# # somehow limit to just the significant variables?
# plot(varImp(rf))

```


```{r}
# LogitAIC PREDICTED V ACTUAL TESTING
# test_predicted <- predict(logit_contacted_test, default_test)
# expected <- default_test$sr_12a_actions_contacted_officials_binary
# 
# table(expected)
# table(test_predicted)
# 
# confusionMatrix(test_predicted, expected)

```

```{r}
# Variable importance

# is there a way to get this to show only the significant variables in the final model?
# ggplot(varImp(logit_contacted_test))
```


```{r, eval=T, echo=F, include=T}
# Variable inflation factors (rule of thumb: under 4 is good)
# vif(logit_contacted_test$finalModel)

```


